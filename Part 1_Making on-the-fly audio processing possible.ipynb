{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac60b08",
   "metadata": {},
   "source": [
    "# Part 1: Making on-the-fly audio processing possible\n",
    "\n",
    "#### Welcome to the part 1 tutorial!\n",
    "\n",
    "We will compare the audio processing time between nnAudio GPU and librosa by using `Google SPEECHCOMMANDS dataset v2 (12 classes) with linear model` in KeyWord Spotting (KWS) task\n",
    "\n",
    "Original dataset has total 35 single wordings. In this KWS task, 10 out of 35 words are chosen ( ‘down’, ‘go’, ‘left’, ‘no’, ‘off’, ‘on’, ‘right’, ‘stop’, ‘up’, ‘yes’). The remaining 25 words are grouped as `class ‘unknown’`. A `class ‘silence’` is created from background noise\n",
    "\n",
    "[Step 1: import related libraries](#Step-1:-import-related-libraries)\\\n",
    "[Step 2: setting up configuration](#Step-2:-setting-up-configuration)\\\n",
    "[Step 3: setting up dataset](#Step-3:-setting-up-dataset)\\\n",
    "[Step 4: data rebalancing](#Step-4:-data-rebalancing)\\\n",
    "[Step 5: data processing and loading](#Step-5:-data-processing-and-loading)\\\n",
    "[Step 6: setting up the Lightning Module](#Step-6:-setting-up-the-Lightning-Module)\n",
    "* [Step 6(i): Lightning Module for Linearmodel_nnAudio](#Step-6(i):-Lightning-Module-for-Linearmodel_nnAudio)\n",
    "* [Step 6(ii): Lightning Module for Linearmodel_librosa](#Step-6(ii):-Lightning-Module-for-Linearmodel_librosa)\n",
    "\n",
    "[Step 7: setting up nnAudio MelSpectrogram](#Step-7:-setting-up-nnAudio-MelSpectrogram)\\\n",
    "[Step 8: setting up the model](#Step-8:-setting-up-the-model)\n",
    "* [Step 8(i): setting up the model with nnAudio](#Step-8(i):-setting-up-the-model-with-nnAudio)\n",
    "* [Step 8(ii): setting up the model with librosa](#Step-8(ii):-setting-up-the-model-with-librosa)\n",
    "\n",
    "[Step 9: training the model for 1 epoch](#Step-9:-training-the-model-for-1-epoch)\n",
    "\n",
    "* [Step 9(i): training the Linearmodel_nnAudio](#Step-9(i):-training-the-Linearmodel_nnAudio)\n",
    "* [Step 9(ii): training the Linearmodel_librosa](#Step-9(ii):-training-the-Linearmodel_librosa)\n",
    "\n",
    "[Conclusion](#Conclusion:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be7ab4e",
   "metadata": {},
   "source": [
    "## Step 1: import related libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries related to PyTorch\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torchaudio \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import WeightedRandomSampler,DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "#Libraries related to dataset\n",
    "from AudioLoader.Speech import SPEECHCOMMANDS_12C #for 12 classes KWS task\n",
    "\n",
    "# Libraries related to PyTorch Lightning\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "\n",
    "# Libraries used in lightning module\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Front-end tool\n",
    "from nnAudio.features.mel import MelSpectrogram, STFT\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda8bc20",
   "metadata": {},
   "source": [
    "## Step 2: setting up configuration\n",
    "\n",
    "Note: If you don't have SPEECHCOMMANDS dataset, you can simply set `download_option= True` to download by using AudioLoader in Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e5941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "gpus = 1\n",
    "batch_size= 100\n",
    "max_epochs = 1\n",
    "check_val_every_n_epoch = 2\n",
    "num_sanity_val_steps = 5\n",
    "\n",
    "data_root= './' # Download the data here\n",
    "download_option= False\n",
    "\n",
    "n_mels= 40 \n",
    "#number of Mel bins\n",
    "\n",
    "input_dim= (n_mels*101)\n",
    "output_dim= 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3789b5b7",
   "metadata": {},
   "source": [
    "## Step 3: setting up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33040bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = SPEECHCOMMANDS_12C(root=data_root,\n",
    "                              url='speech_commands_v0.02',\n",
    "                              folder_in_archive='SpeechCommands',\n",
    "                              download= download_option,subset= 'training') \n",
    "\n",
    "validset = SPEECHCOMMANDS_12C(root=data_root,\n",
    "                              url='speech_commands_v0.02',\n",
    "                              folder_in_archive='SpeechCommands',\n",
    "                              download= download_option,subset= 'validation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c284e8e5",
   "metadata": {},
   "source": [
    "## Step 4: data rebalancing\n",
    "\n",
    "Due to the class imbalance between the ‘silence’(10th class) and ‘unknown’(11th class) class, we re-balance the training set by adjusting the sampling weight for each class during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = [1,1,1,1,1,1,1,1,1,1,4.6,1/17]\n",
    "sample_weights = [0] * len(trainset)\n",
    "#create a list as per length of trainset\n",
    "\n",
    "for idx, (data,rate,label,speaker_id, _) in enumerate(trainset):\n",
    "    class_weight = class_weights[label]\n",
    "    sample_weights[idx] = class_weight\n",
    "#apply sample_weights in each data base on their label class in class_weight\n",
    "\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights),replacement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfd65a8",
   "metadata": {},
   "source": [
    "## Step 5: data processing and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data processing\n",
    "def data_processing(data):\n",
    "    waveforms = []\n",
    "    labels = []\n",
    "    \n",
    "    for batch in data:\n",
    "        waveforms.append(batch[0].squeeze(0)) #after squeeze => (audio_len) tensor # remove batch dim\n",
    "        labels.append(batch[2])      \n",
    "        \n",
    "    waveform_padded = nn.utils.rnn.pad_sequence(waveforms, batch_first=True)  \n",
    "    \n",
    "    output_batch = {'waveforms': waveform_padded, \n",
    "             'labels': torch.tensor(labels),\n",
    "             }\n",
    "    return output_batch\n",
    "\n",
    "#data loading\n",
    "trainloader = DataLoader(trainset,                                \n",
    "                              collate_fn=lambda x: data_processing(x),\n",
    "                                         batch_size=batch_size,sampler=sampler)\n",
    "\n",
    "validloader = DataLoader(validset,                               \n",
    "                              collate_fn=lambda x: data_processing(x),\n",
    "                                         batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f203f680",
   "metadata": {},
   "source": [
    "## Step 6: setting up the Lightning Module\n",
    "\n",
    "### Step 6(i): Lightning Module for Linearmodel_nnAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906b0fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCommand(LightningModule):\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs, spec = self(batch['waveforms']) \n",
    "        #return outputs [2D] for calculate loss, return spec [3D] for visual\n",
    "        loss = self.criterion(outputs, batch['labels'].long())\n",
    "\n",
    "        acc = sum(outputs.argmax(-1) == batch['labels'])/outputs.shape[0] #batch wise\n",
    "        \n",
    "        self.log('Train/acc', acc, on_step=False, on_epoch=True)\n",
    "        self.log('Train/Loss', loss, on_step=False, on_epoch=True)\n",
    "        #log(graph title, take acc as data, on_step: plot every step, on_epch: plot every epoch)\n",
    "        return loss\n",
    " \n",
    "\n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx,\n",
    "                       optimizer_closure, on_tpu, using_native_amp, using_lbfgs):\n",
    "        \n",
    "        optimizer.step(closure=optimizer_closure)\n",
    "        with torch.no_grad():\n",
    "            torch.clamp_(self.mel_layer.mel_basis, 0, 1)\n",
    "        #after optimizer step, do clamp function on mel_basis         \n",
    "\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):               \n",
    "        outputs, spec = self(batch['waveforms'])\n",
    "        loss = self.criterion(outputs, batch['labels'].long())        \n",
    "       \n",
    "        self.log('Validation/Loss', loss, on_step=False, on_epoch=True)                     \n",
    "        output_dict = {'outputs': outputs,\n",
    "                       'labels': batch['labels']}        \n",
    "        return output_dict\n",
    "\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        pred = []\n",
    "        label = []\n",
    "        for output in outputs:\n",
    "            pred.append(output['outputs'])\n",
    "            label.append(output['labels'])\n",
    "        label = torch.cat(label, 0)\n",
    "        pred = torch.cat(pred, 0)\n",
    "        acc = sum(pred.argmax(-1) == label)/label.shape[0]\n",
    "        \n",
    "        self.log('Validation/acc', acc, on_step=False, on_epoch=True)    \n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        model_param = []\n",
    "        for name, params in self.named_parameters():\n",
    "            if 'mel_layer.' in name:\n",
    "                pass\n",
    "            else:\n",
    "                model_param.append(params)          \n",
    "        optimizer = optim.SGD(model_param, lr=1e-3, momentum= 0.9, weight_decay= 0.001)\n",
    "        return [optimizer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0fdcb4",
   "metadata": {},
   "source": [
    "### Step 6(ii): Lightning Module for Linearmodel_librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e1486",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCommand_librosa(LightningModule):\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs, spec = self(batch['waveforms']) \n",
    "        loss = self.criterion(outputs, batch['labels'].long())\n",
    "\n",
    "        acc = sum(outputs.argmax(-1) == batch['labels'])/outputs.shape[0] #batch wise\n",
    "        \n",
    "        self.log('Train/acc', acc, on_step=False, on_epoch=True)\n",
    "        self.log('Train/Loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "     \n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx,\n",
    "                       optimizer_closure, on_tpu, using_native_amp, using_lbfgs):       \n",
    "        optimizer.step(closure=optimizer_closure)       \n",
    "\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):               \n",
    "        outputs, spec = self(batch['waveforms'])\n",
    "        loss = self.criterion(outputs, batch['labels'].long())        \n",
    "       \n",
    "        self.log('Validation/Loss', loss, on_step=False, on_epoch=True)                     \n",
    "        output_dict = {'outputs': outputs,\n",
    "                       'labels': batch['labels']}        \n",
    "        return output_dict\n",
    "\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        pred = []\n",
    "        label = []\n",
    "        for output in outputs:\n",
    "            pred.append(output['outputs'])\n",
    "            label.append(output['labels'])\n",
    "        label = torch.cat(label, 0)\n",
    "        pred = torch.cat(pred, 0)\n",
    "        acc = sum(pred.argmax(-1) == label)/label.shape[0]\n",
    "        \n",
    "        self.log('Validation/acc', acc, on_step=False, on_epoch=True)    \n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        model_param = []\n",
    "        for name, params in self.named_parameters():\n",
    "            if 'mel_layer.' in name:\n",
    "                pass\n",
    "            else:\n",
    "                model_param.append(params)          \n",
    "  \n",
    "        optimizer = optim.SGD(model_param, lr=1e-3, momentum= 0.9, weight_decay= 0.001)\n",
    "        return [optimizer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c765503a",
   "metadata": {},
   "source": [
    "## Step 7: setting up nnAudio MelSpectrogram \n",
    "\n",
    "nnAudio supports the calculation of linear-frequency spectrogram, log-frequency spectrogram, Mel-spectrogram, and Constant Q Transform (CQT). \n",
    "\n",
    "In this tutorial, we will use Mel-spectrogram as an example.You can modify Mel-spectrogram argument from the function below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab9ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_layer = MelSpectrogram(sr=16000, \n",
    "                           n_fft=480,\n",
    "                           win_length=None,\n",
    "                           n_mels=n_mels, \n",
    "                           hop_length=160,\n",
    "                           window='hann',\n",
    "                           center=True,\n",
    "                           pad_mode='reflect',\n",
    "                           power=2.0,\n",
    "                           htk=False,\n",
    "                           fmin=0.0,\n",
    "                           fmax=None,\n",
    "                           norm=1,\n",
    "                           trainable_mel=False,\n",
    "                           trainable_STFT=False,\n",
    "                           verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf02e55",
   "metadata": {},
   "source": [
    "## Step 8: setting up the model\n",
    "Both models take sound files (x) as input. Then we apply `nnAudio.features.mel.MelSpectrogram()` in Linearmodel_nnAudio and `librosa.feature.melspectrogram` in Linearmodel_librosa.\n",
    "\n",
    "For demonstration purposes, we only build a simple model with one linear layer here. `The output of this KWS classification task is in 12 classes`, hence the output size of the layer should be 12.\n",
    "\n",
    "\n",
    "### Step 8(i): setting up the model with nnAudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cdec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linearmodel_nnAudio(SpeechCommand):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.mel_layer = mel_layer       \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.linearlayer = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        #x: 2D [B, 16000]\n",
    "        spec = self.mel_layer(x)  \n",
    "        #spec: 3D [B, F40, T101]\n",
    "        \n",
    "        spec = torch.log(spec+1e-10)\n",
    "        flatten_spec = torch.flatten(spec, start_dim=1) \n",
    "        #flatten_spec: 2D [B, F*T(40*101)] \n",
    "        #start_dim: flattening start from 1st dimention\n",
    "        \n",
    "        out = self.linearlayer(flatten_spec) \n",
    "        #out: 2D [B,number of class(12)] \n",
    "                               \n",
    "        return out, spec \n",
    "\n",
    "model_nnAudo = Linearmodel_nnAudio()\n",
    "model_nnAudo = model_nnAudo.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23ba1de",
   "metadata": {},
   "source": [
    "### Step 8(ii): setting up the model with librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3762ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linearmodel_librosa(SpeechCommand_librosa):\n",
    "    def __init__(self): \n",
    "        super().__init__()       \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.linearlayer = nn.Linear(input_dim, output_dim)   \n",
    "    \n",
    "    def forward(self, x): \n",
    "        #x: 2D [B, 16000]\n",
    "        spec_list =[]\n",
    "        for i in x:\n",
    "            spec = i.cpu().detach().numpy()\n",
    "            spec = librosa.feature.melspectrogram(y=spec,\n",
    "                                                  sr=16000,  \n",
    "                                                  n_fft=480,\n",
    "                                                  win_length=None,\n",
    "                                                  n_mels=n_mels,\n",
    "                                                  hop_length=160,                                 \n",
    "                                                  window='hann', \n",
    "                                                  center=True, \n",
    "                                                  pad_mode='reflect', \n",
    "                                                  power=2.0, \n",
    "                                                  htk=False, \n",
    "                                                  fmin=0.0, \n",
    "                                                  fmax=None,                                 \n",
    "                                                  norm=1,)  \n",
    "            \n",
    "            #append back to batch\n",
    "            spec_list.append(spec)\n",
    "        spec_batch = torch.tensor(spec_list)  #spec_batch: [100, 40, 101]\n",
    "        \n",
    "        spec_batch.cuda()\n",
    "        spec_batch = torch.log(spec_batch+1e-10)\n",
    "        flatten_spec = torch.flatten(spec_batch, start_dim=1).cuda()\n",
    "        #flatten_spec: 2D [B, F*T(40*101)] \n",
    "        #start_dim: flattening start from 1st dimention\n",
    "\n",
    "        out = self.linearlayer(flatten_spec) #out: [B,12]\n",
    "        return out, spec_batch\n",
    "\n",
    "model_librosa = Linearmodel_librosa()\n",
    "model_librosa = model_librosa.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d01b8a",
   "metadata": {},
   "source": [
    "## Step 9: training the model for 1 epoch\n",
    "\n",
    "### Step 9(i): training the Linearmodel_nnAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf34eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(gpus=gpus, max_epochs=max_epochs,\n",
    "    check_val_every_n_epoch= check_val_every_n_epoch,\n",
    "    num_sanity_val_steps=num_sanity_val_steps)\n",
    "\n",
    "trainer.fit(model_nnAudo, trainloader, validloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd28584",
   "metadata": {},
   "source": [
    "### Step 9(ii): training the Linearmodel_librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df50dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(gpus=gpus, max_epochs=max_epochs,\n",
    "    check_val_every_n_epoch= check_val_every_n_epoch,\n",
    "    num_sanity_val_steps=num_sanity_val_steps)\n",
    "\n",
    "trainer.fit(model_librosa, trainloader, validloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e3496",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "The result above shows the computation time of nnAudio GPU and librosa. Librosa took 27mins for one epoch, however **nnAudio GPU only took around 17s to finish one epoch which is 95x faster than librosa!**\n",
    "\n",
    "Next step, let's explore the nnAudio Trainable Basis Functions in Part 2 tutorial - **Part 2_Training a Linear model with nnAudio Trainable Basis Functions**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial",
   "language": "python",
   "name": "tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
