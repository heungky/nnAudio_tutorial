{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f0b9bb",
   "metadata": {},
   "source": [
    "# Part 4: Using more complex non-linear models\n",
    "\n",
    "After following part 1â€“3 of the tutorial, you now have a big picture overview of how to use nnAudio with trainable basis functions.\\\n",
    "In this tutorial, `Broadcasting-residual network (BC_ResNet)` will be used for demonstration on how nnAudio is applied in more complex model. You can simply `replace the code in Step 8` with others model.\n",
    "\n",
    "Same as previous tutorial, `Google SPEECHCOMMANDS dataset v2 (12 classes) will be used`.\n",
    "\n",
    "[Step 1: import related libraries](#Step-1:-import-related-libraries)\\\n",
    "[Step 2: setting up configuration](#Step-2:-setting-up-configuration)\\\n",
    "[Step 3: setting up nnAudio basis functions](#Step-3:-setting-up-nnAudio-basis-functions)\\\n",
    "[Step 4: setting up dataset](#Step-4:-setting-up-dataset)\\\n",
    "[Step 5: data rebalancing](#Step-5:-data-rebalancing)\\\n",
    "[Step 6: data processing and loading](#Step-6:-data-processing-and-loading)\\\n",
    "[Step 7: setting up the lightning module](#Step-7:-setting-up-the-lightning-module)\\\n",
    "[Step 8: setting up model](#Step-8:-setting-up-model)\\\n",
    "[Step 9: training model](#Step-9:-training-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b55681",
   "metadata": {},
   "source": [
    "## Step 1: import related libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec59a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries related to PyTorch\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import WeightedRandomSampler,DataLoader\n",
    "import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# Libraries related to PyTorch Lightning\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "\n",
    "# Libraries used in lightning module\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Libraried related to dataset\n",
    "from AudioLoader.Speech import SPEECHCOMMANDS_12C #for 12 classes KWS task\n",
    "\n",
    "# nnAudio Front-end\n",
    "from nnAudio.features.mel import MelSpectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc46fca0",
   "metadata": {},
   "source": [
    "## Step 2: setting up configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe6b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "gpus = 1\n",
    "batch_size= 100\n",
    "max_epochs = 200\n",
    "check_val_every_n_epoch = 2\n",
    "num_sanity_val_steps = 5\n",
    "\n",
    "data_root= './' # Download the data here\n",
    "download_option= False\n",
    "\n",
    "n_mels= 40 \n",
    "#number of Mel bins\n",
    "\n",
    "output_dim= 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07de7ae",
   "metadata": {},
   "source": [
    "# nnAudio guideline for trainable basis functions \n",
    "```\n",
    "nnAudio.features.mel.MelSpectrogram(trainable_mel= ,trainable_STFT=) \n",
    "```\n",
    "The function above is controlling if mel bases and STFT trainable\n",
    "\n",
    "* A. Both Mel and STFT are non-trainable: \n",
    "`trainable_mel=False, trainable_STFT=False`\n",
    "* B. Mel is trainable while STFT is fixed: \n",
    "`trainable_mel=True, trainable_STFT=False`\n",
    "* C. Mel is fixed while STFT is trainable: \n",
    "`trainable_mel=False, trainable_STFT=True`\n",
    "* D. Both Mel and STFT are trainable:\n",
    "`trainable_mel=True, trainable_STFT=True`\n",
    "\n",
    "## Step 3: setting up nnAudio basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b8cf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_layer = MelSpectrogram(sr=16000, \n",
    "                           n_fft=480,\n",
    "                           win_length=None,\n",
    "                           n_mels=n_mels, \n",
    "                           hop_length=160,\n",
    "                           window='hann',\n",
    "                           center=True,\n",
    "                           pad_mode='reflect',\n",
    "                           power=2.0,\n",
    "                           htk=False,\n",
    "                           fmin=0.0,\n",
    "                           fmax=None,\n",
    "                           norm=1,\n",
    "                           trainable_mel=False,\n",
    "                           trainable_STFT=False,\n",
    "                           verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484cb3e7",
   "metadata": {},
   "source": [
    "## Step 4: setting up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e3e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = SPEECHCOMMANDS_12C(root=data_root,\n",
    "                              url='speech_commands_v0.02',\n",
    "                              folder_in_archive='SpeechCommands',\n",
    "                              download= download_option,subset= 'training') \n",
    "\n",
    "validset = SPEECHCOMMANDS_12C(root=data_root,\n",
    "                              url='speech_commands_v0.02',\n",
    "                              folder_in_archive='SpeechCommands',\n",
    "                              download= download_option,subset= 'validation')\n",
    "\n",
    "testset = SPEECHCOMMANDS_12C(root=data_root,\n",
    "                              url='speech_commands_v0.02',\n",
    "                              folder_in_archive='SpeechCommands',\n",
    "                              download= download_option,subset= 'testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c66cc52",
   "metadata": {},
   "source": [
    "## Step 5: data rebalancing\n",
    "\n",
    "For class weighting, rebalancing silence(10th class) and unknown(11th class) in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cabc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = [1,1,1,1,1,1,1,1,1,1,4.6,1/17]\n",
    "sample_weights = [0] * len(trainset)\n",
    "#create a list as per length of trainset\n",
    "\n",
    "for idx, (data,rate,label,speaker_id, _) in enumerate(trainset):\n",
    "    class_weight = class_weights[label]\n",
    "    sample_weights[idx] = class_weight\n",
    "#apply sample_weights in each data base on their label class in class_weight\n",
    "\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights),replacement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ed8c7a",
   "metadata": {},
   "source": [
    "## Step 6: data processing and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5510dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data processing\n",
    "def data_processing(data):\n",
    "    waveforms = []\n",
    "    labels = []\n",
    "    \n",
    "    for batch in data:\n",
    "        waveforms.append(batch[0].squeeze(0)) #after squeeze => (audio_len) tensor # remove batch dim\n",
    "        labels.append(batch[2])      \n",
    "        \n",
    "    waveform_padded = nn.utils.rnn.pad_sequence(waveforms, batch_first=True)  \n",
    "    \n",
    "    output_batch = {'waveforms': waveform_padded, \n",
    "             'labels': torch.tensor(labels),\n",
    "             }\n",
    "    return output_batch\n",
    "\n",
    "#data loading\n",
    "trainloader = DataLoader(trainset,                                \n",
    "                              collate_fn=lambda x: data_processing(x),\n",
    "                                         batch_size=batch_size,sampler=sampler,num_workers=1)\n",
    "\n",
    "validloader = DataLoader(validset,                               \n",
    "                              collate_fn=lambda x: data_processing(x),\n",
    "                                         batch_size=batch_size,num_workers=1)\n",
    "\n",
    "testloader = DataLoader(testset,   \n",
    "                              collate_fn=lambda x: data_processing(x),\n",
    "                                        batch_size=batch_size,num_workers=1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2061eec4",
   "metadata": {},
   "source": [
    "## Step 7: setting up the lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db981f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCommand(LightningModule):\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs, spec = self(batch['waveforms']) \n",
    "        #return outputs [2D] for calculate loss, return spec [3D] for visual\n",
    "        loss = self.criterion(outputs, batch['labels'].long())\n",
    "\n",
    "        acc = sum(outputs.argmax(-1) == batch['labels'])/outputs.shape[0] #batch wise\n",
    "        \n",
    "        self.log('Train/acc', acc, on_step=False, on_epoch=True)\n",
    "          \n",
    "        self.log('Train/Loss', loss, on_step=False, on_epoch=True)\n",
    "        #log(graph title, take acc as data, on_step: plot every step, on_epch: plot every epoch)\n",
    "        return loss\n",
    "\n",
    "     \n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx,\n",
    "                       optimizer_closure, on_tpu, using_native_amp, using_lbfgs):\n",
    "        \n",
    "        optimizer.step(closure=optimizer_closure)\n",
    "        with torch.no_grad():\n",
    "            torch.clamp_(self.mel_layer.mel_basis, 0, 1)\n",
    "        #after optimizer step, do clamp function on mel_basis \n",
    "        \n",
    "   \n",
    "    def validation_step(self, batch, batch_idx):               \n",
    "        outputs, spec = self(batch['waveforms'])\n",
    "        loss = self.criterion(outputs, batch['labels'].long())        \n",
    "       \n",
    "        self.log('Validation/Loss', loss, on_step=False, on_epoch=True)          \n",
    "\n",
    "        output_dict = {'outputs': outputs,\n",
    "                       'labels': batch['labels']}        \n",
    "        return output_dict\n",
    "\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        pred = []\n",
    "        label = []\n",
    "        for output in outputs:\n",
    "            pred.append(output['outputs'])\n",
    "            label.append(output['labels'])\n",
    "        label = torch.cat(label, 0)\n",
    "        pred = torch.cat(pred, 0)\n",
    "        acc = sum(pred.argmax(-1) == label)/label.shape[0]\n",
    "        \n",
    "        self.log('Validation/acc', acc, on_step=False, on_epoch=True)    \n",
    "        #use the return value from validation_step: output_dict , to calculate the overall accuracy   \n",
    "                              \n",
    "    def test_step(self, batch, batch_idx):               \n",
    "        outputs, spec = self(batch['waveforms'])\n",
    "        loss = self.criterion(outputs, batch['labels'].long())        \n",
    "\n",
    "        self.log('Test/Loss', loss, on_step=False, on_epoch=True)          \n",
    "\n",
    "        output_dict = {'outputs': outputs,\n",
    "                       'labels': batch['labels']}        \n",
    "        return output_dict\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        pred = []\n",
    "        label = []\n",
    "        for output in outputs:\n",
    "            pred.append(output['outputs'])\n",
    "            label.append(output['labels'])\n",
    "        label = torch.cat(label, 0)\n",
    "        pred = torch.cat(pred, 0)\n",
    "        \n",
    "        result_dict = {}\n",
    "        for key in [None, 'micro', 'macro', 'weighted']:\n",
    "            result_dict[key] = {}\n",
    "            p, r, f1, _ = precision_recall_fscore_support(label.cpu(), pred.argmax(-1).cpu(), average=key, zero_division=0)\n",
    "            result_dict[key]['precision'] = p\n",
    "            result_dict[key]['recall'] = r\n",
    "            result_dict[key]['f1'] = f1\n",
    "            \n",
    "        acc = sum(pred.argmax(-1) == label)/label.shape[0]\n",
    "        self.log('Test/acc', acc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        self.log('Test/micro_f1', result_dict['micro']['f1'], on_step=False, on_epoch=True)\n",
    "        self.log('Test/macro_f1', result_dict['macro']['f1'], on_step=False, on_epoch=True)\n",
    "        self.log('Test/weighted_f1', result_dict['weighted']['f1'], on_step=False, on_epoch=True)      \n",
    "        \n",
    "        return result_dict        \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        model_param = []\n",
    "        for name, params in self.named_parameters():\n",
    "            if 'mel_layer.' in name:\n",
    "                pass\n",
    "            else:\n",
    "                model_param.append(params)          \n",
    "\n",
    "        optimizer = optim.SGD(model_param, lr=1e-3, momentum= 0.9, weight_decay= 0.001)\n",
    "        return [optimizer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9dcef1",
   "metadata": {},
   "source": [
    "## Step 8: setting up model \n",
    "You can replace the code below with others model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2cd87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BC_ResNet model\n",
    "class SubSpectralNorm(LightningModule):\n",
    "    def __init__(self, C, S, eps=1e-5):\n",
    "        super(SubSpectralNorm, self).__init__()\n",
    "        self.S = S\n",
    "        self.eps = eps\n",
    "        self.bn = nn.BatchNorm2d(C*S)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: input features with shape {N, C, F, T}\n",
    "        # S: number of sub-bands\n",
    "        N, C, F, T = x.size()\n",
    "        x = x.view(N, C * self.S, F // self.S, T)\n",
    "\n",
    "        x = self.bn(x)\n",
    "        return x.view(N, C, F, T)\n",
    "    \n",
    "class BroadcastedBlock(LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            planes: int,\n",
    "            dilation=1,\n",
    "            stride=1,\n",
    "            temp_pad=(0, 1),\n",
    "    ) -> None:\n",
    "        super(BroadcastedBlock, self).__init__()\n",
    "\n",
    "        self.freq_dw_conv = nn.Conv2d(planes, planes, kernel_size=(3, 1), padding=(1, 0), groups=planes,\n",
    "                                      dilation=dilation,\n",
    "                                      stride=stride, bias=False)\n",
    "        self.ssn1 = SubSpectralNorm(planes, 5)\n",
    "        self.temp_dw_conv = nn.Conv2d(planes, planes, kernel_size=(1, 3), padding=temp_pad, groups=planes,\n",
    "                                      dilation=dilation, stride=stride, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.channel_drop = nn.Dropout2d(p=0.1)\n",
    "        self.swish = nn.SiLU()\n",
    "        self.conv1x1 = nn.Conv2d(planes, planes, kernel_size=(1, 1), bias=False)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        # f2\n",
    "        ##########################\n",
    "        out = self.freq_dw_conv(x)\n",
    "        out = self.ssn1(out)\n",
    "        ##########################\n",
    "\n",
    "        auxilary = out\n",
    "        out = out.mean(2, keepdim=True)  # frequency average pooling\n",
    "\n",
    "        # f1\n",
    "        ############################\n",
    "        out = self.temp_dw_conv(out)\n",
    "        out = self.bn(out)\n",
    "        out = self.swish(out)\n",
    "        out = self.conv1x1(out)\n",
    "        out = self.channel_drop(out)\n",
    "        ############################\n",
    "\n",
    "        out = out + identity + auxilary\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class TransitionBlock(LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            inplanes: int,\n",
    "            planes: int,\n",
    "            dilation=1,\n",
    "            stride=1,\n",
    "            temp_pad=(0, 1),\n",
    "    ) -> None:\n",
    "        super(TransitionBlock, self).__init__()\n",
    "\n",
    "        self.freq_dw_conv = nn.Conv2d(planes, planes, kernel_size=(3, 1), padding=(1, 0), groups=planes,\n",
    "                                      stride=stride,\n",
    "                                      dilation=dilation, bias=False)\n",
    "        self.ssn = SubSpectralNorm(planes, 5)\n",
    "        self.temp_dw_conv = nn.Conv2d(planes, planes, kernel_size=(1, 3), padding=temp_pad, groups=planes,\n",
    "                                      dilation=dilation, stride=stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.channel_drop = nn.Dropout2d(p=0.5)\n",
    "        self.swish = nn.SiLU()\n",
    "        self.conv1x1_1 = nn.Conv2d(inplanes, planes, kernel_size=(1, 1), bias=False)\n",
    "        self.conv1x1_2 = nn.Conv2d(planes, planes, kernel_size=(1, 1), bias=False)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # f2\n",
    "        #############################\n",
    "        out = self.conv1x1_1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.freq_dw_conv(out)\n",
    "        out = self.ssn(out)\n",
    "        #############################\n",
    "        auxilary = out\n",
    "        out = out.mean(2, keepdim=True)  # frequency average pooling\n",
    "\n",
    "        # f1\n",
    "        #############################\n",
    "        out = self.temp_dw_conv(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.swish(out)\n",
    "        out = self.conv1x1_2(out)\n",
    "        out = self.channel_drop(out)\n",
    "        #############################\n",
    "\n",
    "        out = auxilary + out\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    \n",
    "class BCResNet_nnAudio(SpeechCommand):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5, stride=(2, 1), padding=(2, 2))\n",
    "        self.block1_1 = TransitionBlock(16, 8)\n",
    "        self.block1_2 = BroadcastedBlock(8)\n",
    "\n",
    "        self.block2_1 = TransitionBlock(8, 12, stride=(2, 1), dilation=(1, 2), temp_pad=(0, 2))\n",
    "        self.block2_2 = BroadcastedBlock(12, dilation=(1, 2), temp_pad=(0, 2))\n",
    "\n",
    "        self.block3_1 = TransitionBlock(12, 16, stride=(2, 1), dilation=(1, 4), temp_pad=(0, 4))\n",
    "        self.block3_2 = BroadcastedBlock(16, dilation=(1, 4), temp_pad=(0, 4))\n",
    "        self.block3_3 = BroadcastedBlock(16, dilation=(1, 4), temp_pad=(0, 4))\n",
    "        self.block3_4 = BroadcastedBlock(16, dilation=(1, 4), temp_pad=(0, 4))\n",
    "\n",
    "        self.block4_1 = TransitionBlock(16, 20, dilation=(1, 8), temp_pad=(0, 8))\n",
    "        self.block4_2 = BroadcastedBlock(20, dilation=(1, 8), temp_pad=(0, 8))\n",
    "        self.block4_3 = BroadcastedBlock(20, dilation=(1, 8), temp_pad=(0, 8))\n",
    "        self.block4_4 = BroadcastedBlock(20, dilation=(1, 8), temp_pad=(0, 8))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5, groups=20, padding=(0, 2))\n",
    "        self.conv3 = nn.Conv2d(20, 32, 1, bias=False)\n",
    "        self.conv4 = nn.Conv2d(32, output_dim, 1, bias=False)\n",
    "                \n",
    "        self.mel_layer = mel_layer \n",
    "        self.criterion = nn.CrossEntropyLoss()        \n",
    "     \n",
    "\n",
    "    def forward(self, x):        \n",
    "        #x: 2D [Batch_size,16000]\n",
    "        spec = self.mel_layer(x) \n",
    "        #spec: 3D [B,F(40),T]\n",
    "        \n",
    "        spec = torch.log(spec+1e-10)         \n",
    "        spec = spec.unsqueeze(1)\n",
    "        #spec: bcoz conv1 need 4D [B,1,F,T]\n",
    "\n",
    "        out = self.conv1(spec)\n",
    "        out = self.block1_1(out)\n",
    "        out = self.block1_2(out)\n",
    "\n",
    "        out = self.block2_1(out)\n",
    "        out = self.block2_2(out)\n",
    "\n",
    "        out = self.block3_1(out)\n",
    "        out = self.block3_2(out)\n",
    "        out = self.block3_3(out)\n",
    "        out = self.block3_4(out)\n",
    "\n",
    "        out = self.block4_1(out)\n",
    "        out = self.block4_2(out)\n",
    "        out = self.block4_3(out)\n",
    "        out = self.block4_4(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = out.mean(-1, keepdim=True)\n",
    "\n",
    "        out = self.conv4(out)   \n",
    "        #out: 4D [8, 35, 1, 1]\n",
    "        out = out.squeeze(2).squeeze(2)  \n",
    "        #out: 2D        \n",
    "        #crossentropy expect[B, C], so need to squeeze to be 2D\n",
    "\n",
    "        spec = spec.squeeze(1) \n",
    "        #spec: from 4D [B,1,F,T] to 3D [B,F,T]\n",
    "        #the return spec is for plot log_images, so need 3D\n",
    "\n",
    "        return out, spec\n",
    "    \n",
    "model = BCResNet_nnAudio()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80a885e",
   "metadata": {},
   "source": [
    "## Step 9: training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(gpus=gpus, max_epochs=max_epochs,\n",
    "    check_val_every_n_epoch= check_val_every_n_epoch,\n",
    "    num_sanity_val_steps=num_sanity_val_steps)\n",
    "\n",
    "trainer.fit(model, trainloader, validloader)\n",
    "trainer.test(model, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eb2ca4",
   "metadata": {},
   "source": [
    "**You should get few test scores after training the model which include:**\n",
    "\n",
    "* Test/Loss (cross-entropy)\n",
    "* Test/acc (accuracy)\n",
    "* Test/micro_f1\n",
    "* Test/macro_f1\n",
    "* Test/weighted_f1\n",
    "\n",
    "\n",
    "Everytime you train a model, the trained weight will be saved in `./lightning_logs/version_<XX>` folder.\\\n",
    "You can use `Part3_evaluation with nnAudio trainable basis functions` to visualize the result.\n",
    "\n",
    "**Follow the instructions below to visualize your model performance:**\n",
    "1. Simply `replace linear model (Step 7 in part 3 tutorial)` with `your model (step 8 in part 4 tutorail)`.\n",
    "1. Amend the configuration `(Step 2 and step 3 in part 3 tutorial)` if needed.\n",
    "1. Load your trained model weight `(Step 8 & Visualizing the result sections in part 3 tutorial)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4f7107",
   "metadata": {},
   "source": [
    "# Congratulations! This is the end for the keyword spotting (KWS) task with nnAudio GPU audio processing tutorial.\n",
    "We hope this will help you on your way with your own project using nnAudio. Thank you for your support ~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial",
   "language": "python",
   "name": "tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
