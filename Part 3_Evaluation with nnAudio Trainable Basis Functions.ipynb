{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac60b08",
   "metadata": {},
   "source": [
    "# Part 3: Evaluation with nnAudio Trainable Basis Functions\n",
    "\n",
    "### Welcome to the part 3 tutorial!!\n",
    "Since you have trained a linear model with trainable basis functions in part 2 tutorial, now it is time to evaluate the model performance and do some visualization!\\\n",
    "You can use your own model weight or apply the pretrained weight that we have prepared for you (in the `trained_weight_for_tutorial3` folder)\n",
    "\n",
    "### Let's start!\n",
    "\n",
    "[Step 1: import related libraries](#Step-1:-import-related-libraries)\\\n",
    "[Step 2: setting up configuration](#Step-2:-setting-up-configuration)\\\n",
    "[Step 3: setting up nnAudio basis functions](#Step-3:-setting-up-nnAudio-basis-functions)\\\n",
    "[Step 4: loading the dataset](#Step-4:-loading-the-dataset)\\\n",
    "[Step 5: data processing and loading](#Step-5:-data-processing-and-loading)\\\n",
    "[Step 6: setting up the lightning module](#Step-6:-setting-up-the-lightning-module)\\\n",
    "[Step 7: defining the model](#Step-7:-defining-the-model)\\\n",
    "[Step 8: loading pre-trained weight to the model](#Step-8:-loading-pre-trained-weight-to-the-model)\\\n",
    "[Step 9: evaluating the model performance](#Step-9:-evaluating-the-model-performance)\n",
    "\n",
    "[Visualizing the result](#Visualizing-the-result)\n",
    "* [Visualizing the Mel bins](#Visualizing-the-Mel-bins)\n",
    "* [Visualizing the short-time Fourier transform (STFT)](#Visualizing-the-STFT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2655df1",
   "metadata": {},
   "source": [
    "## Step 1: import related libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries related to PyTorch\n",
    "import torch\n",
    "from torch import Tensor \n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Libraries related to PyTorch Lightning\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "\n",
    "# Libraries used for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "#Libraries related to dataset\n",
    "from AudioLoader.Speech import SPEECHCOMMANDS_12C #for 12 classes KWS task\n",
    "from AudioLoader.Speech import idx2name, name2idx\n",
    "\n",
    "# nnAudio Front-end\n",
    "from nnAudio.features.mel import MelSpectrogram, STFT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda8bc20",
   "metadata": {},
   "source": [
    "## Step 2: setting up configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e5941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "gpus = 1\n",
    "batch_size= 100\n",
    "max_epochs = 200\n",
    "check_val_every_n_epoch = 2\n",
    "num_sanity_val_steps = 5\n",
    "\n",
    "data_root= './' # Download the data here\n",
    "download_option= False\n",
    "\n",
    "n_mels= 40 \n",
    "#number of Mel bins\n",
    "\n",
    "input_dim= (n_mels*101)\n",
    "output_dim= 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c765503a",
   "metadata": {},
   "source": [
    "## Step 3: setting up nnAudio basis functions\n",
    "\n",
    "* The model weight inside the trained_weight_for_tutorial2 folder is trained with `setting D`\n",
    "* If you are using your own model trained weight, please match the MelSpectrogram() below with your experiment setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab9ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_layer = MelSpectrogram(sr=16000, \n",
    "                           n_fft=480,\n",
    "                           win_length=None,\n",
    "                           n_mels=n_mels, \n",
    "                           hop_length=160,\n",
    "                           window='hann',\n",
    "                           center=True,\n",
    "                           pad_mode='reflect',\n",
    "                           power=2.0,\n",
    "                           htk=False,\n",
    "                           fmin=0.0,\n",
    "                           fmax=None,\n",
    "                           norm=1,\n",
    "                           trainable_mel=True,\n",
    "                           trainable_STFT=True,\n",
    "                           verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3789b5b7",
   "metadata": {},
   "source": [
    "## Step 4: loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33040bfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testset = SPEECHCOMMANDS_12C(root=data_root,\n",
    "                              url='speech_commands_v0.02',\n",
    "                              folder_in_archive='SpeechCommands',\n",
    "                              download= download_option,subset= 'testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfd65a8",
   "metadata": {},
   "source": [
    "## Step 5: data processing and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data padding\n",
    "def data_processing(data):\n",
    "    waveforms = []\n",
    "    labels = []\n",
    "    \n",
    "    for batch in data:\n",
    "        waveforms.append(batch[0].squeeze(0)) #after squeeze => (audio_len) tensor # remove batch dim\n",
    "        labels.append(batch[2])      \n",
    "        \n",
    "    waveform_padded = nn.utils.rnn.pad_sequence(waveforms, batch_first=True)  \n",
    "    \n",
    "    output_batch = {'waveforms': waveform_padded, \n",
    "             'labels': torch.tensor(labels),\n",
    "             }\n",
    "    return output_batch\n",
    "\n",
    "#data loading\n",
    "testloader = DataLoader(testset,   \n",
    "                              collate_fn=lambda x: data_processing(x),\n",
    "                                        batch_size=batch_size, num_workers =1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f203f680",
   "metadata": {},
   "source": [
    "## Step 6: setting up the lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906b0fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCommand(LightningModule):     \n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx,\n",
    "                       optimizer_closure, on_tpu, using_native_amp, using_lbfgs):\n",
    "        \n",
    "        optimizer.step(closure=optimizer_closure)\n",
    "        with torch.no_grad():\n",
    "            torch.clamp_(self.mel_layer.mel_basis, 0, 1)\n",
    "        #after optimizer step, do clamp function on mel_basis\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):               \n",
    "        outputs, spec = self(batch['waveforms'])\n",
    "        loss = self.criterion(outputs, batch['labels'].long())        \n",
    "\n",
    "        self.log('Test/Loss', loss, on_step=False, on_epoch=True)          \n",
    "        \n",
    "        output_dict = {'outputs': outputs,\n",
    "                       'labels': batch['labels']}        \n",
    "        return output_dict\n",
    "    \n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        pred = []\n",
    "        label = []\n",
    "        for output in outputs:\n",
    "            pred.append(output['outputs'])\n",
    "            label.append(output['labels'])\n",
    "        label = torch.cat(label, 0)\n",
    "        pred = torch.cat(pred, 0)\n",
    "        \n",
    "        result_dict = {}\n",
    "        for key in [None, 'micro', 'macro', 'weighted']:\n",
    "            result_dict[key] = {}\n",
    "            p, r, f1, _ = precision_recall_fscore_support(label.cpu(), pred.argmax(-1).cpu(), average=key, zero_division=0)\n",
    "            result_dict[key]['precision'] = p\n",
    "            result_dict[key]['recall'] = r\n",
    "            result_dict[key]['f1'] = f1\n",
    "            \n",
    "        barplot(result_dict, 'precision', figsize=(4,6))\n",
    "        barplot(result_dict, 'recall',figsize=(4,6))\n",
    "        barplot(result_dict, 'f1',figsize=(4,6))\n",
    "            \n",
    "        acc = sum(pred.argmax(-1) == label)/label.shape[0]\n",
    "        self.log('Test/acc', acc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        self.log('Test/micro_f1', result_dict['micro']['f1'], on_step=False, on_epoch=True)\n",
    "        self.log('Test/macro_f1', result_dict['macro']['f1'], on_step=False, on_epoch=True)\n",
    "        self.log('Test/weighted_f1', result_dict['weighted']['f1'], on_step=False, on_epoch=True)\n",
    "        \n",
    "        cm = plot_confusion_matrix(label.cpu(),\n",
    "                                   pred.argmax(-1).cpu(),\n",
    "                                   name2idx.keys(),\n",
    "                                   title='Test: Confusion matrix',\n",
    "                                   normalize=False)                    \n",
    "        return result_dict\n",
    "\n",
    "    \n",
    "def plot_confusion_matrix(correct_labels,\n",
    "                          predict_labels,\n",
    "                          labels,\n",
    "                          title='Confusion matrix',\n",
    "                          normalize=False):\n",
    "    ''' \n",
    "    Parameters:\n",
    "        correct_labels                  : These are your true classification categories.\n",
    "        predict_labels                  : These are you predicted classification categories\n",
    "        labels                          : This is a lit of labels which will be used to display the axix labels\n",
    "        title='Confusion matrix'        : Title for your matrix\n",
    "        tensor_name = 'MyFigure/image'  : Name for the output summay tensor\n",
    "    Returns:\n",
    "        summary: TensorFlow summary \n",
    "    Other itema to note:\n",
    "        - Depending on the number of category and the data , you may have to modify the figzie, font sizes etc. \n",
    "        - Currently, some of the ticks dont line up due to rotations.\n",
    "    '''\n",
    "    cm = confusion_matrix(correct_labels, predict_labels, labels=range(len(labels)))\n",
    "    if normalize:\n",
    "        cm = cm.astype('float')*10 / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm = np.nan_to_num(cm, copy=True)\n",
    "        cm = cm.astype('int')\n",
    "\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4.5, 4.5), dpi=160, facecolor='w', edgecolor='k')\n",
    "    fig.suptitle('confusion_matrix',fontsize=7)\n",
    "    im = ax.imshow(cm, cmap='Oranges')\n",
    "\n",
    "    classes = [re.sub(r'([a-z](?=[A-Z])|[A-Z](?=[A-Z][a-z]))', r'\\1 ', x) for x in labels]\n",
    "    #classes = ['\\n'.join(l) for l in classes]\n",
    "\n",
    "    tick_marks = np.arange(len(classes))\n",
    "\n",
    "    ax.set_xlabel('Predicted', fontsize=7)\n",
    "    ax.set_xticks(tick_marks)\n",
    "    c = ax.set_xticklabels(classes, fontsize=5, rotation=0,  ha='center')\n",
    "    ax.xaxis.set_label_position('bottom')\n",
    "    ax.xaxis.tick_bottom()\n",
    "\n",
    "    ax.set_ylabel('True Label', fontsize=7)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_yticklabels(classes, fontsize=5, va ='center')\n",
    "    ax.yaxis.set_label_position('left')\n",
    "    ax.yaxis.tick_left()\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        ax.text(j, i, format(cm[i, j], 'd') if cm[i,j]!=0 else '.', horizontalalignment=\"center\", fontsize=6, verticalalignment='center', color= \"black\")\n",
    "    fig.set_tight_layout(True)\n",
    "\n",
    "    return fig\n",
    "\n",
    "def barplot(result_dict, title, figsize=(4,12), minor_interval=0.2, log=False):\n",
    "    fig, ax = plt.subplots(1,1, figsize=figsize)\n",
    "    metric = {}\n",
    "    for idx, item in enumerate(result_dict[None][title]):\n",
    "        metric[idx2name[idx]] = item\n",
    "    xlabels = list(metric.keys())\n",
    "    values = list(metric.values())\n",
    "    if log:\n",
    "        values = np.log(values)\n",
    "    ax.barh(xlabels, values)\n",
    "    ax.tick_params(labeltop=True, labelright=False)\n",
    "    ax.xaxis.grid(True, which='minor')\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(minor_interval))\n",
    "    ax.set_ylim([-1,len(xlabels)])\n",
    "    ax.set_title(title)\n",
    "    ax.grid(axis='x')\n",
    "    ax.grid(b=True, which='minor', linestyle='--')\n",
    "    fig.savefig(f'{title}.png', bbox_inches='tight')\n",
    "    fig.tight_layout() # prevent edge from missing\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf02e55",
   "metadata": {},
   "source": [
    "## Step 7: defining the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cdec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linearmodel_nnAudio(SpeechCommand):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.mel_layer = mel_layer       \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.linearlayer = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    \n",
    "    def forward(self, x): \n",
    "        #x: 2D [B, 16000]\n",
    "        spec = self.mel_layer(x)  \n",
    "        #spec: 3D [B, F40, T101]\n",
    "        \n",
    "        spec = torch.log(spec+1e-10)\n",
    "        \n",
    "        flatten_spec = torch.flatten(spec, start_dim=1) \n",
    "        #flatten_spec: 2D [B, F*T(40*101)] \n",
    "        #start_dim: flattening start from 1st dimention\n",
    "        \n",
    "        out = self.linearlayer(flatten_spec) \n",
    "        #out: 2D [B,number of class(12)]                               \n",
    "        return out, spec \n",
    "\n",
    "model_nnAudo = Linearmodel_nnAudio()\n",
    "model_nnAudo = model_nnAudo.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261242f8",
   "metadata": {},
   "source": [
    "## Step 8: loading pre-trained weight to the model\n",
    "Everytime you train a model after part 2 tutorial, the trained weight will be saved in `lightning_logs` folder.\n",
    "\n",
    "We have prepared a checkpoint file inside the `trained_weight_for_tutorial2` folder which use for demostration in the following.\n",
    "\n",
    "The detail of trained weight:\n",
    "* Linearmodel in keyword spotting task\n",
    "* Setting D: Both Mel and STFT are trainable ( `trainable_mel=True, trainable_STFT=True`)\n",
    "* `n_mels = 40`\n",
    "* Test/acc = 45.1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f97109",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_weight= model_nnAudo.load_from_checkpoint('./trained_weight_for_tutorial3/Linearmodel_nnAudio-speechcommand-mel=trainable-STFT=trainable/version_1/checkpoints/last.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d01b8a",
   "metadata": {},
   "source": [
    "## Step 9: evaluating the model performance \n",
    "Model performance on the KWS task can be evaluated using the following metrics on the test set:\n",
    "* Test/Loss (cross-entropy)\n",
    "* Test/acc (accuracy)\n",
    "* F1 matrix (F1 scores)\n",
    "* Confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf34eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(gpus=gpus, max_epochs=max_epochs,\n",
    "    check_val_every_n_epoch= check_val_every_n_epoch,\n",
    "    num_sanity_val_steps=num_sanity_val_steps)\n",
    "\n",
    "trainer.test(trained_weight, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6714a219",
   "metadata": {},
   "source": [
    "# Visualizing the result \n",
    "\n",
    "* [Visualizing the Mel bins](#Visualizing-the-Mel-bins)\n",
    "* [Visualizing the short-time Fourier transform (STFT)](#Visualizing-the-STFT)\n",
    "\n",
    "We can visualise some of the learned kernels within our 1st layer of nnAudio as the weights are stored in our checkpoint file.\n",
    "\n",
    "The structure inside the checkpoint file looks like this:\n",
    "```\n",
    "weight=torch.load('xxxx/checkpoints/xxxx.ckpt')\n",
    "├── epoch\n",
    "├── global_step\n",
    "├── pytorch-lightning_version\n",
    "│     \n",
    "├── state_dict\n",
    "│     ├─ mel_layer.mel_basis\n",
    "│     ├─ mel_layer.stft.wsin\n",
    "│     ├─ mel_layer.stft.wcos\n",
    "│     ├─ mel_layer.stft.window_mask   \n",
    "│     ├─ linearlayer.weight\n",
    "│     ├─ linearlayer.bias\n",
    "│     │\n",
    "│     \n",
    "├── callbacks\n",
    "├── optimizer_states\n",
    "├── lr_schedulers\n",
    "```\n",
    "\n",
    "`torch.load('xxxx/checkpoints/xxxx.ckpt')` is a dictionary, its keys can be checked in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b854c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight=torch.load('trained_weight_for_tutorial3/Linearmodel_nnAudio-speechcommand-mel=trainable-STFT=trainable/version_1/checkpoints/last.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d08cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c6604e",
   "metadata": {},
   "source": [
    "`'state_dict'` is one of the dictionary key in the checkpoint file, it is an `OrderedDict` which including the **trained weight for basis functions (Mel bins, STFT) and layer weight (linear layer in this case)**.\\\n",
    "Keys for the 'state_dict' (OrderedDict) can be checked in the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa0fbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight['state_dict'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a0a6d",
   "metadata": {},
   "source": [
    "## Visualizing the Mel bins\n",
    "The shape of `mel_layer.mel_basis` should be `[n_mels, (n_fft/2+1)]`, whereby n_mels is number of Mel bin and n_fft refers to the length of the windowed signal after padding with zeros.\\\n",
    "In this tutorial example, the shape of mel_layer.mel_basis is `[40,241]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_bins = weight['state_dict']['mel_layer.mel_basis']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd091a",
   "metadata": {},
   "source": [
    "**Individual Mel bin can be shown in the following:**\n",
    "```python\n",
    "plt.plot(mel_bins[i].cpu().detach().numpy())\n",
    "```\n",
    "Simply replace `i` with `the index of Mel base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76943821",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mel_bins[0].cpu().detach().numpy())\n",
    "plt.title('Amplitude of an individual Mel bin')\n",
    "plt.xlabel('No. of frequency bin')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d31b6f",
   "metadata": {},
   "source": [
    "**40 Mel bases can be shown in the following:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1d8106",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mel_bins:\n",
    "    plt.plot(i.cpu().detach().numpy()) \n",
    "    \n",
    "plt.title('Amplitude of 40 Mel bins')\n",
    "plt.xlabel('No. of frequency bin')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107ffc5",
   "metadata": {},
   "source": [
    "## Visualizing the STFT\n",
    "Shape of `'mel_layer.stft.wsin'` and `'mel_layer.stft.wcos'` should be `[(n_fft/2+1),1,n_fft]`\\\n",
    "In the case of `linear model in keyword spotting task`, their shape should be  `[241, 1, 480]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f60bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsin = weight['state_dict']['mel_layer.stft.wsin']\n",
    "wcos = weight['state_dict']['mel_layer.stft.wcos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8ec766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing STFT_wsin\n",
    "fig, axes = plt.subplots(2,2)\n",
    "for ax, kernel_num in zip(axes.flatten(), [2,10,20,50]):\n",
    "    ax.plot(wsin[kernel_num,0].cpu())\n",
    "    ax.set_ylim(-1,1)\n",
    "    fig.suptitle('Visualizing STFT_wsin')\n",
    "    \n",
    "plt.setp(axes[-1, :], xlabel='No. of sample')\n",
    "plt.setp(axes[:, 0], ylabel='Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c1845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing STFT_wcos\n",
    "fig, axes = plt.subplots(2,2)\n",
    "for ax, kernel_num in zip(axes.flatten(), [2,10,20,50]):\n",
    "    ax.plot(wcos[kernel_num,0].cpu())\n",
    "    ax.set_ylim(-1,1)\n",
    "    fig.suptitle('Visualizing STFT_wcos')\n",
    "    \n",
    "plt.setp(axes[-1, :], xlabel='No. of sample')\n",
    "plt.setp(axes[:, 0], ylabel='Amplitude')   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891467e0",
   "metadata": {},
   "source": [
    "# Congratulations!  You have finished the Part 3 tutorial.\n",
    "Feel free to move on to `Part 4: Using nnAudio Trainable Basis Functions with more complex non-linear models`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial",
   "language": "python",
   "name": "tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
